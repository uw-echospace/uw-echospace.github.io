<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Echospace @ UW</title><link>https://uw-echospace.github.io/</link><atom:link href="https://uw-echospace.github.io/index.xml" rel="self" type="application/rss+xml"/><description>Echospace @ UW</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 03 Oct 2023 00:00:00 -0800</lastBuildDate><image><url>https://uw-echospace.github.io/images/icon_hu9a3e81eca2a83564e549c2cdc32b0637_27049_512x512_fill_lanczos_center_2.png</url><title>Echospace @ UW</title><link>https://uw-echospace.github.io/</link></image><item><title>Machine learning in fisheries acoustics</title><link>https://uw-echospace.github.io/project/2021-hake-workflow/</link><pubDate>Sat, 01 May 2021 17:02:36 -0800</pubDate><guid>https://uw-echospace.github.io/project/2021-hake-workflow/</guid><description>&lt;p>Active acoustic data collected with scientific echosounders from acoustic surveys have become essential for stock assessment in fisheries resources management. Over the past three decades, a wide suite of physics-based acoustic scattering models were developed to allow translating acoustic observations to biological quantities, such as biomass and abundance, for different marine organisms. In parallel, quantitative scientific echosounders have progressed from specialized equipment to one of the standard instruments on fisheries survey vessels.&lt;/p>
&lt;p>To take full advantage of these large and complex new datasets, in this project we aim to combine the development of machine learning methodology with a cloud-based workflow to accelerate the extraction of biological information from fisheries acoustic data. This project is in close collaboration with the
&lt;a href="https://www.fisheries.noaa.gov/west-coast/sustainable-fisheries/fisheries-engineering-and-acoustic-technologies-team" target="_blank" rel="noopener">Fisheries Engineering and Acoustics Technology (FEAT) team&lt;/a> at the NOAA Fisheries
&lt;a href="https://www.fisheries.noaa.gov/about/northwest-fisheries-science-center" target="_blank" rel="noopener">Northwest Fisheries science center (NWFSC)&lt;/a> and uses data collected in the past 20 years off the west coast of the U.S. from the
&lt;a href="https://www.fisheries.noaa.gov/west-coast/science-data/joint-us-canada-integrated-ecosystem-and-pacific-hake-acoustic-trawl-survey" target="_blank" rel="noopener">Joint U.S.-Canada Integrated Ecosystem and Pacific Hake Acoustic-Trawl Survey&lt;/a> (aka the &amp;ldquo;Hake survey&amp;rdquo;).&lt;/p>
&lt;p>&lt;strong>Funding agency&lt;/strong>: NOAA Fisheries&lt;/p></description></item><item><title>Scalable, cloud-native processing of water column sonar data</title><link>https://uw-echospace.github.io/project/2021-cloud-workflows/</link><pubDate>Sun, 24 Oct 2021 16:56:36 -0800</pubDate><guid>https://uw-echospace.github.io/project/2021-cloud-workflows/</guid><description>&lt;p>Scientists commonly use active sonar systems to collect data about mid-trophic level animals like zooplankton and small fish, which play an important role in the marine ecosystems. Echosounders, or fish-finders, are high-frequency sonar systems that emit pulses of sound and record the reflections from animals, the seabed, and other objects. These instruments have been proven to be more efficient and effective for collecting data over a large survey area or a long time period than many other sampling methods, such as underwater imaging and net trawls. This technology has been widely adopted by the ocean science and commercial fishing communities and more recently has been integrated with autonomous vehicles, resulting in a massive amount of data. However, these datasets can be difficult to analyze and are often underutilized. We will address this issue by adopting and advancing data standards, developing a streamlined data processing workflow, and integrating open-source software tools that capitalize on recent advancements in cloud computing technologies to efficiently transform large quantities of ocean sonar data into information that is useful for exploring, monitoring, and managing living marine resources.&lt;/p>
&lt;p>&lt;strong>Funding agency&lt;/strong>: NOAA Office of Ocean Exploration and Research
&lt;a href="https://oceanexplorer.noaa.gov/news/oer-updates/2021/fy21-ffo-schedule.html" target="_blank" rel="noopener">FY2021 grants&lt;/a>&lt;/p></description></item><item><title>EchoPro workflow modernization</title><link>https://uw-echospace.github.io/project/2021-echopro/</link><pubDate>Tue, 25 Jan 2022 17:02:36 -0800</pubDate><guid>https://uw-echospace.github.io/project/2021-echopro/</guid><description>&lt;p>The
&lt;a href="https://www.fisheries.noaa.gov/west-coast/sustainable-fisheries/fisheries-engineering-and-acoustic-technologies-team" target="_blank" rel="noopener">Fisheries Engineering and Acoustics Technology (FEAT) team&lt;/a> at the NOAA Fisheries
&lt;a href="https://www.fisheries.noaa.gov/about/northwest-fisheries-science-center" target="_blank" rel="noopener">Northwest Fisheries Science Center (NWFSC)&lt;/a> collaborates with
&lt;a href="https://www.dfo-mpo.gc.ca/index-eng.htm" target="_blank" rel="noopener">Fisheries and Oceans Canada (DFO) - Pacific Region&lt;/a> to estimate total biomass of
&lt;a href="https://www.fisheries.noaa.gov/species/pacific-whiting" target="_blank" rel="noopener">Pacific hake&lt;/a> by incorporating acoustic and biological trawl data from the
&lt;a href="https://www.fisheries.noaa.gov/west-coast/science-data/joint-us-canada-integrated-ecosystem-and-pacific-hake-acoustic-trawl-survey" target="_blank" rel="noopener">Joint U.S.-Canada Integrated Ecosystem and Pacific Hake Acoustic-Trawl Survey&lt;/a> (aka the &amp;ldquo;Hake survey&amp;rdquo;).
These biomass estimates are the inputs for the stock assessment of hake and need to be completed in an efficient and timely manner after the survey. The biomass estimates are currently produced by a suite of Matlab scripts operated by a single user, and the analysis procedures are not easily adaptable by other FEAT/DFO team members. The central objective of this project is to provide a well-documented open-source Python software package that contains the core computational functionality of the current Matlab EchoPro program and provides basic visualization of the analysis results. The new software package will contain an Application Programming Interface (API) that can be invoked in a reproducible manner and can accept configuration of key analysis parameters and other run-time setups via a machine and human-readable configuration file.&lt;/p>
&lt;p>&lt;strong>Funding agency&lt;/strong>: NOAA Fisheries, NOAA NWFSC&lt;/p></description></item><item><title>HakeIGP: Hake biological data integration</title><link>https://uw-echospace.github.io/project/2021-hake-igp/</link><pubDate>Tue, 25 Jan 2022 17:02:36 -0800</pubDate><guid>https://uw-echospace.github.io/project/2021-hake-igp/</guid><description>&lt;p>Conventional acoustic surveys utilize biological data collected with temporally and spatially co-located midwater and/or bottom trawls to validate their observations, i.e., acoustic-trawl (AT) data. To take advantage of the recent technological advances in collecting acoustic data via platforms different from the traditional ship-based systems, such as uncrewed marine systems (UxSs), we need to seek alternative strategies that can provide crucial biological information. Through the Hake biological data integration project (HakeIGP), the echospace team is assisting the
&lt;a href="https://www.fisheries.noaa.gov/west-coast/sustainable-fisheries/fisheries-engineering-and-acoustic-technologies-team" target="_blank" rel="noopener">Fisheries Engineering and Acoustics Technology (FEAT) team&lt;/a> at the NOAA Fisheries
&lt;a href="https://www.fisheries.noaa.gov/about/northwest-fisheries-science-center" target="_blank" rel="noopener">Northwest Fisheries Science Center (NWFSC)&lt;/a> as they explore the applicability of using biological information for
&lt;a href="https://www.fisheries.noaa.gov/species/pacific-whiting" target="_blank" rel="noopener">Pacific hake&lt;/a> from bottom trawl (BT) surveys and the at-sea hake observer programs (OP) in both U.S. and Canadian waters to augment coast-wide hake acoustic surveys using UxSs. This work will involve the integration of the AT, BT, and OP biological data from 2003 to 2019 into a NWFSC-hosted database with a unified format. This will enable the quantitative evaluation of similarities and differences among the three data sources and assessment of the associated variability. The resulting software and workflow will facilitate the integration of all three data sources into a unified format and enable research to answer two important questions: (1) Can we use biological data from alternative sources to provide effective and satisfactory biomass estimates or biomass indices for UxSs-based acoustic surveys? (2) Can we extract new biological information on hake by analyzing the integrated fishery–dependent (OP) and fishery–independent (AT and BT) data?&lt;/p>
&lt;p>&lt;strong>Funding agency&lt;/strong>: NOAA Fisheries, NOAA NWFSC&lt;/p></description></item><item><title>ADCP-equipped underwater glider as a distributed biological sensing tool</title><link>https://uw-echospace.github.io/project/2020-glider-adcp/</link><pubDate>Tue, 01 Sep 2020 17:02:36 -0800</pubDate><guid>https://uw-echospace.github.io/project/2020-glider-adcp/</guid><description>&lt;p>Mid-trophic level animals, such as zooplankton and fish, are keystone organisms in the marine ecosystem and play a critical role in the economy and our food supply chain. However, our understanding of these animals, particularly those in the pelagic zones, is severely limited, due to the lack of tools that cab . This gap of knowledge has greatly impeded our ability in making informed policy decisions to support sustainable resource management. The root cause of this problem is the lack of tools that can collect information about these animals at large temporal and spatial scales comparable to other physical, chemical, and lower-trophic biological (e.g., chlorophyll) oceanographic variables.&lt;/p>
&lt;p>Gliders have provided unparalleled mobile, persistent access to deep, remote ocean environments at a fraction of the cost of a research vessel. Taking advantage of this unique capability, in this project we aim to develop sampling strategies and data analysis methodologies to enable distributed long-term observation of mid-trophic marine organisms using
&lt;a href="https://apl.uw.edu/project/project.php?id=seaglider_auv" target="_blank" rel="noopener">Seagliders&lt;/a> equipped with acoustic Doppler current profilers (ADCPs).&lt;/p>
&lt;p>&lt;strong>Funding agency&lt;/strong>: NOAA Office of Ocean Exploration and Research
&lt;a href="https://oceanexplorer.noaa.gov/news/oer-updates/2020/fy20-ffo-schedule.html" target="_blank" rel="noopener">FY2020 grants&lt;/a>&lt;/p></description></item><item><title>Echopype</title><link>https://uw-echospace.github.io/software/echopype/</link><pubDate>Sun, 01 Nov 2020 14:50:00 -0800</pubDate><guid>https://uw-echospace.github.io/software/echopype/</guid><description>&lt;p>&lt;strong>&lt;code>Echopype&lt;/code>&lt;/strong> is an open-source Python package aimed at
&lt;a href="https://echopype.readthedocs.io/en/latest/why.html" target="_blank" rel="noopener">enhancing the interoperability and scalability&lt;/a>
in processing ocean sonar data for biological information.&lt;/p>
&lt;p>I started building this package in early 2018 when I couldn&amp;rsquo;t find an affordable tool that
allow easy access and manipulation of echosounder data collected by
different sonar models.&lt;/p>
&lt;p>For the latest updates, check out our repo at: &lt;a href="https://github.com/OSOceanAcoustics/echopype">https://github.com/OSOceanAcoustics/echopype&lt;/a>.&lt;/p>
&lt;p>Check out my talk at SciPy 2019 that discussed the goals and philosophy of echopype:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/qboH7MyHrpU" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p></description></item><item><title>Pattern discovery from long-term echosounder time series</title><link>https://uw-echospace.github.io/project/2019-ooi-mtx-decomp/</link><pubDate>Tue, 01 Jan 2019 14:50:00 -0800</pubDate><guid>https://uw-echospace.github.io/project/2019-ooi-mtx-decomp/</guid><description>&lt;p>&lt;strong>Funding agency&lt;/strong>: National Science Foundation
&lt;a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1849930&amp;amp;HistoricalAwards=false" target="_blank" rel="noopener">Award #1849930&lt;/a>&lt;/p></description></item><item><title>Echo Statistics</title><link>https://uw-echospace.github.io/software/echo-stat-tutorial/</link><pubDate>Sat, 01 Dec 2018 00:00:00 -0800</pubDate><guid>https://uw-echospace.github.io/software/echo-stat-tutorial/</guid><description>&lt;p>For a 2018 tutorial I published with
&lt;a href="https://www2.whoi.edu/staff/tstanton/" target="_blank" rel="noopener">Tim Stanton&lt;/a> and
&lt;a href="https://www.linkedin.com/in/kyungmin-baik-098156149/" target="_blank" rel="noopener">Kyungmin Baik&lt;/a> in
the Journal of the Acoustical Society of America (JASA):&lt;/p>
&lt;p>&lt;strong>Echo statistics associated with discrete scatterers: A tutorial on physics-based methods&lt;/strong>. JASA 144(6): 3124–3171; &lt;a href="https://doi.org/10.1121/1.5052255">https://doi.org/10.1121/1.5052255&lt;/a>&lt;/p>
&lt;p>we provided the Matlab code to reproduce all figures in two forms:&lt;/p>
&lt;ul>
&lt;li>a &lt;em>frozen&lt;/em> version archived with the paper, and&lt;/li>
&lt;li>a GitHub
&lt;a href="https://github.com/leewujung/echo-stats-tutorial" target="_blank" rel="noopener">repository&lt;/a> minted with a
&lt;a href="https://doi.org/10.5281/zenodo.2458776" target="_blank" rel="noopener">DOI from Zenodo&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>This way we can keep the code &amp;ldquo;alive&amp;rdquo; on GitHub but also has a convenient snapshot of the code at the time of the tutorial publication.&lt;/p></description></item><item><title>Modeling target search by echolocating toothed whales</title><link>https://uw-echospace.github.io/project/2018-muri-echolocation/</link><pubDate>Mon, 01 Jan 2018 14:50:00 -0800</pubDate><guid>https://uw-echospace.github.io/project/2018-muri-echolocation/</guid><description>&lt;p>Echolocating animals effortlessly navigate, hunt, and interact with their environment, despite cluttered and noisy return signals. Blind expert human echolocators prove that this capacity does not depend exclusively on biological specializations unique to particular species. This project is an integrated component of a larger collaborative project (see funding below) focused on active sensing in echolocating marine mammals and humans. The overarching goal of the interdisciplinary team is to use both toothed whales (odontocetes) and humans as model systems to identify the neural mechanisms that extract echo-acoustic information and the brain networks that build robust, invariant representations of auditory objects in complex auditory scenes. The UW-APL component of the project involves modeling the echolocation-based target search by toothed whales as an information-seeking behavior by extending the &lt;em>infotaxis&lt;/em> algorithm originally formulated in moth odor tracking problems into an &lt;em>active sensing&lt;/em> context. In parallel with the theoretical work, we also conducted an echolocation target discrimination experiment with a harbor porpoise to guide further model development.&lt;/p>
&lt;p>&lt;strong>Funding agency&lt;/strong>: Office of Naval Research &amp;ndash; Multidisciplinary University Research Initiative (MURI)&lt;/p></description></item><item><title>Code of Conduct &amp; What we value</title><link>https://uw-echospace.github.io/values/2023-10-coc-values/</link><pubDate>Tue, 03 Oct 2023 00:00:00 -0800</pubDate><guid>https://uw-echospace.github.io/values/2023-10-coc-values/</guid><description>&lt;h3 id="why">Why&lt;/h3>
&lt;ul>
&lt;li>We respect and want to take advantage of the diversity of background and expertise in the group.&lt;/li>
&lt;li>We also want to create an optimal learning and research environment for everyone.&lt;/li>
&lt;/ul>
&lt;h3 id="code-of-conduct">Code of Conduct&lt;/h3>
&lt;p>Echospace is dedicated to providing a harassment-free experience for everyone, regardless of gender, gender identity and expression, age, sexual orientation, disability, physical appearance, body size, race, or religion (or lack thereof). We do not tolerate harassment of group members in any form. Sexual language and imagery is not appropriate for any group venue, including meetings, presentations, or discussions, and in both physical and online spaces.&lt;/p>
&lt;h3 id="what-we-value">What we value&lt;/h3>
&lt;ul>
&lt;li>Be proactive
&lt;ul>
&lt;li>Say hello to people you meet at work (in the morning when you come to work, etc)&lt;/li>
&lt;li>Be on time and respect other’s time, communicate if you don’t have time&lt;/li>
&lt;li>Don’t be afraid of being wrong and ask, and answer to questions from others kindly&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Be open to change&lt;/li>
&lt;li>Communication
&lt;ul>
&lt;li>Respect other person’s background may be different from yours&lt;/li>
&lt;li>Be welcoming to others’ perspectives and what makes them tick, what excites them&lt;/li>
&lt;li>Don’t assume people know or don’t know what you’re talking about, it is ok and often better to just ask&lt;/li>
&lt;li>When there are different opinions, try to convince others by reasoning, and avoid being dismissive because of others’ career stages or rank&lt;/li>
&lt;li>Strive to communicate, both when we don’t think we understand what the other person is saying and when we think we are not being understood (misunderstandings)&lt;/li>
&lt;li>Acknowledge contributions from others&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Ask questions and provide feedback kindly
&lt;ul>
&lt;li>Use “we” instead of “you”&lt;/li>
&lt;li>Use sandwich method: start with positive statement, follow with the comment “have you consider XYZ”, and end with positive affirmation/offer support&lt;/li>
&lt;li>Provide constructive feedback, elaborate on your comments, and be willing to explain more&lt;/li>
&lt;li>Value listening to others, and asking respectful questions&lt;/li>
&lt;li>&lt;strong>Provide positive feedback too!&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Group interaction and collaboration
&lt;ul>
&lt;li>Have clear expectations and communicate openly, to avoid last minute surprises&lt;/li>
&lt;li>Ensure everyone has the opportunity to participate both online and in person&lt;/li>
&lt;li>Give opportunities to others to speak first&lt;/li>
&lt;li>Make accommodations for personal emergencies&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Help each other grow
&lt;ul>
&lt;li>Provide opportunities for peer mentoring&lt;/li>
&lt;li>Sharing opportunities in the group that others might be interested in (or would want to be involved in)&lt;/li>
&lt;li>Enable and encourage growth by everyone in the group, in directions that align with what they’re interested in&lt;/li>
&lt;li>Be comfortable bringing up issues or obstacles that hold you back from getting work done&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>A Summer of Refactoring Echoshader!</title><link>https://uw-echospace.github.io/2023/09/18/a-summer-of-refactoring-echoshader/</link><pubDate>Mon, 18 Sep 2023 00:00:00 -0800</pubDate><guid>https://uw-echospace.github.io/2023/09/18/a-summer-of-refactoring-echoshader/</guid><description>&lt;p>&lt;em>Echospace recruited contributor
&lt;a href="https://github.com/ldr426" target="_blank" rel="noopener">Dingrui Lei&lt;/a> in 2023 to refactor an echosounder data interactive visualization package called
&lt;a href="https://github.com/OSOceanAcoustics/echoshader" target="_blank" rel="noopener">echoshader&lt;/a>.&lt;/em>&lt;/p>
&lt;hr>
&lt;h1 id="my-2023-summer-internship-with-echoshader-a-dive-into-advanced-ocean-sonar-data-visualization">My 2023 Summer Internship with Echoshader: A Dive into Advanced Ocean Sonar Data Visualization&lt;/h1>
&lt;p>Author:
&lt;a href="mailto:leidingrui426@gmail.com">Dingrui Lei&lt;/a>&lt;/p>
&lt;p>Ref 1:
&lt;a href="https://docs.google.com/presentation/d/1HmL2-luVmA9T5HfS3L1kBu8c7dDHo75znwaS-8YlTSE/edit#slide=id.p" target="_blank" rel="noopener">Slides&lt;/a> of presentation&lt;/p>
&lt;p>Ref 2:
&lt;a href="https://echoshader--140.org.readthedocs.build/en/140/intro.html" target="_blank" rel="noopener">Docs&lt;/a> for this version&lt;/p>
&lt;p>Hello, readers! I&amp;rsquo;m excited to share my summer internship experience working on the fascinating project, Echoshader. This Python package, designed to enhance the visualization of ocean sonar data, has been my focus this summer. While I won&amp;rsquo;t be delving into technical jargon, I&amp;rsquo;ll give you a glimpse of my journey, the challenges I faced, and the accomplishments achieved during my internship. The prototype was built during
&lt;a href="https://summerofcode.withgoogle.com/programs/2022/organizations/ioos" target="_blank" rel="noopener">GSoC 2022&lt;/a>.&lt;/p>
&lt;h2 id="echoshader-bridging-the-gap-in-ocean-sonar-data-visualization">Echoshader: Bridging the Gap in Ocean Sonar Data Visualization&lt;/h2>
&lt;p>Before I jump into the technical details, let&amp;rsquo;s take a moment to understand the significance of ocean sonar systems. These systems, including echosounders, are the unsung heroes of marine research. They help scientists study marine life by emitting sound waves and analyzing the echoes they bounce back. Think of it as an underwater ultrasound for the ocean. The data generated from these systems is invaluable for monitoring and conserving our marine ecosystems.&lt;/p>
&lt;p>Echoshader, our summer project, aims to make this data more accessible and interactive. It&amp;rsquo;s like a powerful toolset that enables scientists and researchers to visualize and analyze ocean sonar data effortlessly. But let&amp;rsquo;s get into the nitty-gritty of my experience.&lt;/p>
&lt;h2 id="building-the-echoshader-a-structured-journey">Building the Echoshader: A Structured Journey&lt;/h2>
&lt;p>My summer project was all about creating and refining the Echoshader package. This package is the backbone of our mission, providing oceanographers and researchers with the tools they need to visualize and understand ocean sonar data. Here&amp;rsquo;s how I structured my work:&lt;/p>
&lt;h3 id="1-the-echoshader-class-a-controller-for-visualization">1. The Echoshader Class: A Controller for Visualization&lt;/h3>
&lt;p>At the heart of Echoshader lies the Echoshader class. This class is like the conductor of an orchestra, coordinating user interactions, data updates, and visualizations. My task was to make sure this class was robust and user-friendly.&lt;/p>
&lt;p>I defined the class and set up initial values and interactive widgets. These widgets allow users to tweak parameters and explore data interactively.&lt;/p>
&lt;h3 id="2-callbacks-and-streams-making-it-interactive">2. Callbacks and Streams: Making It Interactive&lt;/h3>
&lt;p>Echoshader needed to be interactive, allowing users to explore data dynamically. This required creating callback methods and stream objects. These elements connected user interactions to visualization updates, making the whole experience smooth and intuitive.
&lt;img width="594" alt="image" src="https://github.com/ldr426/add-ldr426-page/assets/56751303/472ca1bf-4ec3-4e27-82db-20dba3f7fa58">&lt;/p>
&lt;h3 id="3-extending-xarray-with-accessors-a-new-level-of-functionality">3. Extending &lt;code>Xarray&lt;/code> with Accessors: A New Level of Functionality&lt;/h3>
&lt;p>One of the exciting challenges I encountered was extending &lt;code>xarray&lt;/code>&amp;rsquo;s functionality using accessors. This means adding custom methods and functionality to &lt;code>xarray&lt;/code> objects, without cluttering the code with custom functions. We created a custom &amp;ldquo;eshader&amp;rdquo; accessor, which allowed us to take echogram visualization to the next level.&lt;/p>
&lt;h2 id="a-glimpse-into-echogram-visualization">A Glimpse into Echogram Visualization&lt;/h2>
&lt;p>Echogram visualization is where the magic happens. It&amp;rsquo;s not just about pretty pictures; it&amp;rsquo;s about gaining insights into marine life and ecosystems.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Echograms for Identifying Fish&lt;/strong>: Fisheries scientists rely on echograms to identify fish aggregations, scrolling through data collected on ships to assess populations.&lt;/li>
&lt;li>&lt;strong>Echograms for Observing Zooplankton&lt;/strong>: Oceanographers use echograms to observe zooplankton movements in mooring data over extended periods.&lt;/li>
&lt;li>&lt;strong>Tricolor Echograms&lt;/strong>: The &amp;ldquo;tricolor&amp;rdquo; echogram helps distinguish different fish species, thanks to its clever mapping of three frequencies to RGB colors.&lt;/li>
&lt;/ul>
&lt;img width="613" alt="single_frequency_echogram" src="https://github.com/ldr426/add-ldr426-page/assets/11621647/a51a6a76-c73d-46c1-84dd-8df643438f07">
&lt;img width="613" alt="tricolor_echogram" src="https://github.com/ldr426/add-ldr426-page/assets/11621647/0ba62c35-5dd0-41db-9225-db0290be1215">
&lt;h2 id="tracking-and-curtain-visualization">Tracking and Curtain Visualization&lt;/h2>
&lt;p>One of the most exciting aspects of Echoshader is tracking and curtain visualization. It&amp;rsquo;s like having a GPS for underwater data.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Echogram-Control Mode&lt;/strong>: Visualizing data on a map helps assess fish associations with environmental variables.&lt;/li>
&lt;li>&lt;strong>Track-Control Mode&lt;/strong>: Highlighting ship track sections on the map while viewing corresponding echograms offers precise insights into marine life at specific locations.&lt;/li>
&lt;li>&lt;strong>Curtain Visualization&lt;/strong>: Representing longer data sections as curtains provides a broader spatial perspective on fish aggregations.
&lt;img width="609" alt="image" src="https://github.com/ldr426/add-ldr426-page/assets/56751303/dab25ce8-bba3-4062-85e9-4ce3231172fc">&lt;/li>
&lt;/ul>
&lt;img width="502" alt="image" src="https://github.com/ldr426/add-ldr426-page/assets/56751303/2a1a33df-c639-4b53-9df2-ae2523cd3901">
&lt;h2 id="histograms-and-statistics-tables-tools-for-deeper-analysis">Histograms and Statistics Tables: Tools for Deeper Analysis&lt;/h2>
&lt;p>Histograms and statistics tables are essential for fisheries scientists.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Focused Analysis&lt;/strong>: Scientists can zoom in on specific data sections to examine volume backscattering strength (Sv) distribution and understand the types of fish present.&lt;/li>
&lt;li>&lt;strong>Multi-Channel Comparisons&lt;/strong>: Comparing Sv distributions across multiple echosounder channels helps determine fish aggregation composition, offering valuable insights into the ecosystem.
&lt;img width="722" alt="image" src="https://github.com/ldr426/add-ldr426-page/assets/56751303/623ca032-cab8-4fe0-8f37-57774a64e1b0">&lt;/li>
&lt;/ul>
&lt;h2 id="in-conclusion-an-incredible-summer-journey">In Conclusion: An Incredible Summer Journey&lt;/h2>
&lt;p>My summer internship with Echoshader has been a remarkable journey. I&amp;rsquo;ve had the privilege of contributing to a project to advance oceanographic research and fisheries science. Echoshader isn&amp;rsquo;t just a package; it&amp;rsquo;s a gateway to uncovering the secrets of our oceans.&lt;/p>
&lt;p>If you&amp;rsquo;re curious about ocean sonar data or want to explore the world of marine life, Echoshader is your partner in discovery. Feel free to reach out if you have questions or want to join us on this exciting journey. Until next time, happy exploring!&lt;/p></description></item><item><title>Investigation of duty cycles in passive acoustic bat monitoring</title><link>https://uw-echospace.github.io/talk/202305-urp-symposium/</link><pubDate>Fri, 19 May 2023 15:30:00 +0200</pubDate><guid>https://uw-echospace.github.io/talk/202305-urp-symposium/</guid><description/></item><item><title>Postdoc position for machine learning in fisheries/ocean acoustics for ecological applications</title><link>https://uw-echospace.github.io/position/2022-08-22-postdoc/</link><pubDate>Mon, 22 Aug 2022 00:00:00 -0800</pubDate><guid>https://uw-echospace.github.io/position/2022-08-22-postdoc/</guid><description>&lt;h3 id="location">Location&lt;/h3>
&lt;p>Seattle, WA, USA&lt;/p>
&lt;h3 id="deadline">Deadline&lt;/h3>
&lt;p>Open until filled. Review of materials will start immediately.&lt;/p>
&lt;h3 id="description">Description&lt;/h3>
&lt;p>We are looking for a highly motivated early-career scientist to join the
&lt;a href="https://uw-echospace.github.io/" target="_blank" rel="noopener">Echospace&lt;/a> group at the University of Washington to conduct research at the intersection of fisheries/ocean acoustics and machine learning. The postdoc will develop computational methods to extract information from large volumes of ocean acoustic data. The initial project will focus on automatic identification of fish and zooplankton in echosounder data collected by ships, with expansions to integrate additional active/passive acoustic and environmental data streams from collocated moorings in the highly productive
&lt;a href="https://tos.org/oceanography/article/why-is-the-northern-end-of-the-california-current-system-so-productive" target="_blank" rel="noopener">northern California Current System&lt;/a>. The project is in close collaboration with the NOAA Northwest Fisheries Science Center. The successful candidate will have a wide range of opportunities to develop new projects, build integrated skills across physics-based models, data-driven methods, and cloud computing, and interact with the vibrant ocean sciences and data science communities at UW and in Seattle.&lt;/p>
&lt;p>The expected start date of this position is in the fall/winter of 2022. The initial appointment period is one year, with extension contingent on satisfactory performance and funding availability (also see Limitations on Appointment below). Promotion following the completion of the appointment is possible subject to demonstration of an interest and the ability to develop independent research initiatives during the appointment. The candidate is encouraged to apply for the
&lt;a href="https://escience.washington.edu/uw-data-science-postdoctoral-fellow/" target="_blank" rel="noopener">UW Data Science Postdoctoral Fellowship&lt;/a> in parallel with this position.&lt;/p>
&lt;p>The
&lt;a href="https://uw-echospace.github.io/" target="_blank" rel="noopener">UW Echospace&lt;/a> group is a highly collaborative research group based jointly at the
&lt;a href="https://www.apl.washington.edu/" target="_blank" rel="noopener">Applied Physics Laboratory (APL)&lt;/a> and
&lt;a href="https://escience.washington.edu/" target="_blank" rel="noopener">eScience Institute&lt;/a>. Our research centers around acoustic ocean sensing and data science, and spans a broad spectrum from development of computational methods and open-source software to joint analysis of acoustic observations and ocean environmental variables. We are committed to provide a supportive environment for group members to grow and contribute to the acoustics, oceanography, and data science communities.&lt;/p>
&lt;p>For other opportunities to work with us, see
&lt;a href="https://uw-echospace.github.io/position/fellowships/" target="_blank" rel="noopener">this page&lt;/a>.&lt;/p>
&lt;h3 id="qualifications">Qualifications&lt;/h3>
&lt;p>The candidate will hold a PhD degree (earned or nearing completion of) in one of the following fields: acoustics, oceanography, fisheries, statistics, or a related field in computational sciences. They will have a track record of first-author, peer-reviewed publication(s) in related domains and experience programming in Python, R, Matlab, or similar. Familiarity with ocean/fisheries acoustics or experience applying machine learning methods to scientific problems are preferred. They are expected to have excellent communication skills, the ability and desire to work in a team environment, and a strong interest in interdisciplinary research approaches.&lt;/p>
&lt;h3 id="application-instructions">Application Instructions&lt;/h3>
&lt;p>Please submit your application to Drs. Wu-Jung Lee (&lt;a href="mailto:leewj@uw.edu">leewj@uw.edu&lt;/a>) and Valentina Staneva (&lt;a href="mailto:vms16@uw.edu">vms16@uw.edu&lt;/a>) with the following materials: 1) CV including a list of publication(s); 2) cover letter with brief description of present and future research interests; 3) a list of three references and their contact information. Informal inquiries are welcome.&lt;/p>
&lt;hr>
&lt;h4 id="_limitations-on-appointment_">&lt;em>Limitations on Appointment&lt;/em>&lt;/h4>
&lt;p>&lt;em>In compliance with regulations at University of Washington, cumulative length of postdoctoral appointment may not exceed 5 years, including postdoctoral experience(s) at other institutions. Postdoctoral Scholars are engaged in full-time mentored advanced training to enhance professional skills and research independence, and perform primarily research and scholarship under the direction and supervision of University faculty mentors.&lt;/em>&lt;/p>
&lt;p>&lt;em>Postdoctoral scholars are represented by UAW 4121 and are subject to the collective bargaining agreement, unless agreed exclusion criteria apply. For more information, please visit the University of Washington Labor Relations website (UAW Contract).&lt;/em>&lt;/p>
&lt;h4 id="_equal-employment-opportunity-statement_">&lt;em>Equal Employment Opportunity Statement&lt;/em>&lt;/h4>
&lt;p>&lt;em>University of Washington is an affirmative action and equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, national origin, sex, sexual orientation, marital status, pregnancy, genetic information, gender identity or expression, age, disability, or protected veteran status.&lt;/em>&lt;/p>
&lt;h4 id="_commitment-to-diversity_">&lt;em>Commitment to Diversity&lt;/em>&lt;/h4>
&lt;p>_The University of Washington is committed to building diversity among its faculty, librarian, staff, and student communities, and articulates that commitment in the UW Diversity Blueprint (&lt;a href="http://www.washington.edu/diversity/diversity-blueprint/)">http://www.washington.edu/diversity/diversity-blueprint/)&lt;/a>. Additionally, the University&amp;rsquo;s Faculty Code recognizes faculty efforts in research, teaching and/or service that address diversity and equal opportunity as important contributions to a faculty member&amp;rsquo;s academic profile and responsibilities (&lt;a href="https://www.washington.edu/admin/rules/policies/FCG/FCCH24.html#2432)._">https://www.washington.edu/admin/rules/policies/FCG/FCCH24.html#2432)._&lt;/a>&lt;/p></description></item><item><title>Hello from Dingrui Lei, GSoC contributor of Echoshader!</title><link>https://uw-echospace.github.io/2022/07/28/hello-from-dingrui-lei-gsoc-contributor-of-echoshader/</link><pubDate>Thu, 28 Jul 2022 00:00:00 -0800</pubDate><guid>https://uw-echospace.github.io/2022/07/28/hello-from-dingrui-lei-gsoc-contributor-of-echoshader/</guid><description>&lt;p>&lt;em>Echospace collaborates with the
&lt;a href="https://ioos.us/" target="_blank" rel="noopener">Integrated Ocean Observing Systems (IOOS)&lt;/a> in the
&lt;a href="https://summerofcode.withgoogle.com/" target="_blank" rel="noopener">Google Summer of Code (GSoC)&lt;/a> program in 2022 to jump start an echosounder data interactive visualization package called
&lt;a href="https://github.com/OSOceanAcoustics/echoshader" target="_blank" rel="noopener">echoshader&lt;/a>.&lt;/em>&lt;/p>
&lt;p>&lt;em>
&lt;a href="https://github.com/ldr426" target="_blank" rel="noopener">Dingrui Lei&lt;/a> is our great GSoC contributor, and our very own
&lt;a href="author/don-setiawan">Don Setiawan&lt;/a> is the primary mentor.&lt;/em>&lt;/p>
&lt;hr>
&lt;p>My name is Dingrui Lei and I am a new graduate student at Rice University. My experience has given me a broader understanding of how computer science knowledge can solve engineering problems and facilitate new tech development. I’d like to utilize my computer science knowledge to solve engineering problems.&lt;/p>
&lt;p>Before contacting the
&lt;a href="https://ioos.us/" target="_blank" rel="noopener">IOOS&lt;/a> community, I read the article &amp;ldquo;
&lt;a href="https://storymaps.arcgis.com/stories/e245977def474bdba60952f30576908f" target="_blank" rel="noopener">Understanding Our Ocean with Water-Column Sonar Data&lt;/a>,&amp;rdquo; and an introduction to the project
&lt;a href="https://uw-echospace.github.io/software/echopype/" target="_blank" rel="noopener">echopype&lt;/a>. Sonar is very intriguing to me, it can continuously detect the activities of sea creatures in the dimension of space and time. The depth of fish clusters changing with solar radiation really made me see the splendid usefulness of sonar data.&lt;/p>
&lt;p>&lt;img src="https://ioos.us/images/IOOS_Emblem_Tertiary_B_RGB.png" alt="The IOOS Logo - The U.S. Integrated Ocean Observing System (IOOS)">&lt;/p>
&lt;p>One of the main focuses of the
&lt;a href="https://uw-echospace.github.io/author/echospace/" target="_blank" rel="noopener">Echospace&lt;/a> team is sampling and interpretation of ocean acoustic data.
&lt;a href="https://github.com/OSOceanAcoustics/echopype" target="_blank" rel="noopener">Echopype&lt;/a> sits in the middle, extracts raw data from the cloud or file server, converts them to netCDF or Zarr, and performs denoising and calibration. Another job is to interpret, where I give my effort to build a library called echoshader that can help oceanographers discover certain patterns from it.
&lt;a href="https://github.com/OSOceanAcoustics/echoshader" target="_blank" rel="noopener">Echoshader&lt;/a>, an open source project, aims to enhance the ability to interactively visualize large amounts of cloud-based data to accelerate the data exploration and discovery process. Ocean sonar data are generated from echopype, which handles the normalization, preprocessing and organization of echo data. Echoshader will be developed in parallel with the ongoing development of echopype.&lt;/p>
&lt;p>As a participant of GSoC, I am developing the main APIs of echoshader based on the
&lt;a href="https://holoviz.org/" target="_blank" rel="noopener">HoloViz&lt;/a> suite of tools, test configuration for using echoshader widgets in Panel dashboards, and create Jupyter notebooks to demo use of the combination of tools.&lt;/p>
&lt;p>&lt;img src="https://miro.medium.com/max/1400/1*xQEm58a7c_g1Go9G5NMyuw.jpeg" alt="Deploying Panel (Holoviz) dashboards using Heroku Container Registry | by Ali Shahid | Towards Data Science">&lt;/p>
&lt;p>Before starting coding, I read lots of documents to find the most suitable tool. Although there are many excellent and fantastic visualizing libraries with Python, such as plotly and bokh, they can not process xarray directly, which is a kind of multidimensional labeled data massively used in echopype. Then I locked my eyes on HoloViz ecosystem, whose tools and examples generally work with any Python standard data types (lists, dictionaries, etc.), plus Pandas or Dask DataFrames and NumPy, Xarray, or Dask arrays.
After determining which type of tool to use, I began to read a user guide about HoloViz libraries. There are several libraries mainly used in echoshader: hvplot, Holoviews, GeoViews and Panel. Hvplot and HoloViews declare objects for instantly visualizable data, building Bokeh plots from convenient high-level specifications. GeoViews visualize geographic data corresponding to ship survey datasets. Panel assembles grams and control widgets from these different libraries into a layout which could be displayed in a Jupyter notebook and in a standalone servable dashboard. In addition to HoloViz libraries, PyVista and other libraries are involved for 3D extension, which also fit well in panel layout. Also, benchmarking and doc work are required for each module.
Below are some screenshots of the different visualization functionalities I am developing:&lt;/p>
&lt;p>&lt;img src="https://user-images.githubusercontent.com/15334215/186999651-76081a29-11f8-4d37-b3a9-fca0ad49a03c.png" alt="2d_echogram">&lt;/p>
&lt;p>&lt;img src="https://user-images.githubusercontent.com/15334215/186999662-ba744a49-b02e-4451-a716-f8c8df654053.png" alt="tracks">&lt;/p>
&lt;p>&lt;img src="https://user-images.githubusercontent.com/15334215/186999678-2bf77985-aab3-42f8-88f9-1f2c78d3b2eb.png" alt="curtain">&lt;/p>
&lt;p>Although the project is not difficult, there are some other challenges I face. Learning Git and Github is a prerequisite for me to participate in open source projects for the first time. It is also my first time to collaborate with an English-speaking team. I had difficulty reading and writing English documents, not to mention, communicating. Fortunately, the mentors, Wu-jung, Don, Valentina, Brandon and Emilio are all kind and warmhearted, willing to give me suggestions and guidance.&lt;/p>
&lt;p>I really recommend future GSoC participants select the IOOS organization and echospace team as your target and exploit your ability and talent to contribute to the community. Water is extremely significant for holding an adequate food supply and a productive environment for all living organisms. So working here can not just improve your coding and teamwork capability, but also create a beautiful tomorrow for ourselves and our Mother Earth.&lt;/p></description></item><item><title>Opportunities at Echospace</title><link>https://uw-echospace.github.io/position/fellowships/</link><pubDate>Mon, 25 Jul 2022 00:00:00 -0800</pubDate><guid>https://uw-echospace.github.io/position/fellowships/</guid><description>&lt;h2 id="general-information">General information&lt;/h2>
&lt;p>We welcome researchers and students with diverse backgrounds to come work with us in Echospace! If you are interested but do not find a specific position posted, feel free to reach out to us directly. Please include a cover letter and a CV/resume when initiating the contact, so that we have a better idea what you are looking for.&lt;/p>
&lt;h2 id="fellowship-opportunities">Fellowship opportunities&lt;/h2>
&lt;p>Below is a list of fellowship opportunities within and outside of UW. Relevant areas include but are not limited to fisheries and ocean acoustics, animal echolocation (bats and dolphins), marine ecology, and environmental data science. Feel free to reach out to us for questions and discussion.&lt;/p>
&lt;h3 id="postdoc-fellowships">Postdoc fellowships&lt;/h3>
&lt;h4 id="university-of-washington">University of Washington&lt;/h4>
&lt;ul>
&lt;li>
&lt;a href="https://ap.washington.edu/ahr/position-details/?job_id=99111" target="_blank" rel="noopener">APL SEED Postdoctoral Scholar Program&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://cicoes.uw.edu/education/postdoc-program/" target="_blank" rel="noopener">Cooperative Institute for Climate, Ocean, and Ecosystem Studies (CICOES) postdoctoral fellowship&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://www.wrfseattle.org/grants/wrf-postdoctoral-fellowships/" target="_blank" rel="noopener">Washington Research Foundation (WRF) Postdoctoral Fellowship&lt;/a>&lt;/li>
&lt;/ul>
&lt;h4 id="external">External&lt;/h4>
&lt;ul>
&lt;li>
&lt;a href="https://beta.nsf.gov/funding/opportunities/ocean-sciences-postdoctoral-research-fellowships-oce-prf-0" target="_blank" rel="noopener">NSF Ocean Sciences (OCE) Postdoctoral Research Fellowships (OCE-PRF)&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://beta.nsf.gov/funding/opportunities/ear-postdoctoral-fellowships-ear-pf" target="_blank" rel="noopener">NSF Division of Earth Sciences (EAR) Postdoctoral Fellowships (EAR-PF)&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://acousticalsociety.org/fellowships-and-scholarships/" target="_blank" rel="noopener">Acoustical Society of America (ASA) Hunt Postdoctoral Research fellowship&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="graduate-fellowships">Graduate fellowships&lt;/h3>
&lt;ul>
&lt;li>
&lt;a href="https://www.nsfgrfp.org/" target="_blank" rel="noopener">NSF Graduate Research Fellowships Program (GRFP)&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="undergraduate-fellowships">Undergraduate fellowships&lt;/h3>
&lt;ul>
&lt;li>
&lt;a href="https://expd.uw.edu/mge/apply/research/" target="_blank" rel="noopener">Mary Gates Research Scholarship&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Understanding echoes</title><link>https://uw-echospace.github.io/talk/202205-asa-keynote/</link><pubDate>Mon, 23 May 2022 16:10:00 +0200</pubDate><guid>https://uw-echospace.github.io/talk/202205-asa-keynote/</guid><description/></item><item><title>Discerning behavioral habits of echolocating bats using acoustical and computational methods</title><link>https://uw-echospace.github.io/talk/202205-urp-symposium/</link><pubDate>Fri, 20 May 2022 15:45:00 +0200</pubDate><guid>https://uw-echospace.github.io/talk/202205-urp-symposium/</guid><description/></item><item><title>Updates from Echopype developers: changes and roadmap</title><link>https://uw-echospace.github.io/talk/202204-wgfast-echopype/</link><pubDate>Wed, 27 Apr 2022 18:10:00 +0200</pubDate><guid>https://uw-echospace.github.io/talk/202204-wgfast-echopype/</guid><description/></item><item><title>Summarizing low-dimensional patterns in long-term echosounder time series from the U.S. Ocean Observatories Initiative network</title><link>https://uw-echospace.github.io/talk/202204-wgfast-ooi-nmf/</link><pubDate>Mon, 25 Apr 2022 18:10:00 +0200</pubDate><guid>https://uw-echospace.github.io/talk/202204-wgfast-ooi-nmf/</guid><description/></item><item><title>Beluga whale (Delphinapterus leucas) acoustic foraging behavior and applications for long term monitoring</title><link>https://uw-echospace.github.io/publication/2021-castellote-etal-plosone-beluga-foraging-monitoring/</link><pubDate>Tue, 30 Nov 2021 09:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/publication/2021-castellote-etal-plosone-beluga-foraging-monitoring/</guid><description/></item><item><title>Echopype: A Python library for interoperable and scalable processing of water column sonar data for biological information</title><link>https://uw-echospace.github.io/publication/2021-lee-etal-echopype/</link><pubDate>Sat, 30 Oct 2021 09:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/publication/2021-lee-etal-echopype/</guid><description/></item><item><title>Scalable, interoperable processing of water column sonar data for biological applications using the echopype Python package</title><link>https://uw-echospace.github.io/talk/202110-ioos-dmac/</link><pubDate>Thu, 28 Oct 2021 15:00:00 -0500</pubDate><guid>https://uw-echospace.github.io/talk/202110-ioos-dmac/</guid><description/></item><item><title>Building a toolbox for studying marine ecology using large ocean sonar datasets</title><link>https://uw-echospace.github.io/talk/202110-uw-data-sci/</link><pubDate>Tue, 05 Oct 2021 16:30:00 -0700</pubDate><guid>https://uw-echospace.github.io/talk/202110-uw-data-sci/</guid><description/></item><item><title>Compact representation of temporal processes in echosounder time series via matrix decomposition</title><link>https://uw-echospace.github.io/publication/2020-lee-staneva-jasa-tsnmf/</link><pubDate>Mon, 30 Nov 2020 09:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/publication/2020-lee-staneva-jasa-tsnmf/</guid><description/></item><item><title>Echo statistics associated with discrete scatterers: A tutorial on physics-based methods</title><link>https://uw-echospace.github.io/publication/2018-stanton-etal-jasa-echo-stat-tutorial/</link><pubDate>Thu, 06 Dec 2018 09:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/publication/2018-stanton-etal-jasa-echo-stat-tutorial/</guid><description/></item><item><title>Macroscopic observations of diel fish movements around a shallow water artificial reef using a mid-frequency horizontal-looking sonar</title><link>https://uw-echospace.github.io/publication/2018-lee-etal-jasa-trex-fish/</link><pubDate>Tue, 18 Sep 2018 09:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/publication/2018-lee-etal-jasa-trex-fish/</guid><description/></item><item><title>Tongue-driven sonar beam steering by a lingual-echolocating fruit bat</title><link>https://uw-echospace.github.io/publication/2017-lee-etal-plosbio-rousettus-bp/</link><pubDate>Fri, 15 Dec 2017 09:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/publication/2017-lee-etal-plosbio-rousettus-bp/</guid><description/></item><item><title>News</title><link>https://uw-echospace.github.io/news/</link><pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/news/</guid><description>
&lt;p>&lt;strong>[09/26/2023]&lt;/strong> YeonJoon has been selected as a &lt;a href="https://escience.washington.edu/people/postdoctoral-fellows/">UW Data Science Postdoctoral Fellow&lt;/a>.&lt;/p>
&lt;p>&lt;strong>[05/19/2023]&lt;/strong> Aditya presented his ongoing research on the effects of subsampling for passive acoustic monitoring of bats at &lt;a href="https://expo.uw.edu/expo/apply/676/proceedings">UW&amp;rsquo;s 26th Annual Undergraduate Research Symposium&lt;/a>.&lt;/p>
&lt;p>&lt;strong>[12/16/2022]&lt;/strong> Aditya has been awarded the &lt;a href="https://expd.uw.edu/mge/apply/research/">Mary Gates Research Scholarship&lt;/a> for his research on passive acoustic monitoring of bats in the Union Bay Natural Area!&lt;/p>
&lt;p>&lt;strong>[08/15/2022]&lt;/strong> Many of us in Echospace and alumnus Derya are hosting the &lt;a href="https://oceanhackweek.github.io/ohw22/index.html">OceanHackWeek 2022&lt;/a> &lt;a href="https://oceanhackweek.github.io/ohw22/seattle/index.html">Northwest satellite&lt;/a> this week!&lt;/p>
&lt;p>&lt;strong>[07/01/2022]&lt;/strong> We welcome &lt;a href="https://uw-echospace.github.io/author/yeonjoon-cheong">Dr. YeonJoon Cheong&lt;/a> to join Echospace as a postdoc scholar!&lt;/p>
&lt;p>&lt;strong>[05/27/2022]&lt;/strong> We have released a &lt;a href="https://echopype.readthedocs.io/en/stable/whats-new.html">new, major version of echopype, 0.6.0&lt;/a>. There are significant breaking changes, but also significant improvements in convention adherence, consistency across sensors, and dataset documentation.&lt;/p>
&lt;p>&lt;strong>[05/23/2022]&lt;/strong> Wu-Jung will be giving the keynote lecture on &amp;ldquo;Understanding Echoes&amp;rdquo; in the &lt;a href="https://acousticalsociety.org/asa-meetings/#KL">ASA Denver meeting&lt;/a>.&lt;/p>
&lt;p>&lt;strong>[05/20/2022]&lt;/strong> Aditya gave a talk on using machine learning to monitor bats in &lt;a href="https://expo.uw.edu/expo/apply/635/proceedings">UW&amp;rsquo;s 25th Annual Undergraduate Research Symposium&lt;/a>.&lt;/p>
&lt;p>&lt;strong>[04/27/2022]&lt;/strong> Wu-Jung and Emilio gave two talks on &lt;a href="https://uw-echospace.github.io/talk/202204-wgfast-echopype">echopype updates and roadmap&lt;/a> in the 2022 WGFAST meeting and the 2022 NOAA NCEI Water Column Sonar Data Archive workshop.&lt;/p>
&lt;p>&lt;strong>[04/25/2022]&lt;/strong> Valentina gave a talk on &lt;a href="https://uw-echospace.github.io/talk/202204-wgfast-ooi-nmf">analyzing OOI echosounder data using matrix decomposition&lt;/a> in the 2022 WGFAST meeting.&lt;/p>
&lt;p>&lt;strong>[11/30/2021]&lt;/strong> New paper &lt;a href="https://doi.org/10.1371/journal.pone.0260485">&amp;ldquo;Beluga whale (&lt;em>Delphinapterus leucas&lt;/em>) acoustic foraging behavior and applications for long term monitoring&amp;rdquo;&lt;/a> was published in PLOS One!&lt;/p>
&lt;p>&lt;strong>[10/30/2021]&lt;/strong> New preprint &lt;a href="https://arxiv.org/abs/2111.00187">&amp;ldquo;Echopype: A Python library for interoperable and scalable processing of water column sonar data for biological information&amp;rdquo;&lt;/a> was posted on arXiv!&lt;/p>
&lt;p>&lt;strong>[10/28/2021]&lt;/strong> Emilio and Wu-Jung gave the IOOS DMAC webinar on &lt;a href="https://uw-echospace.github.io/talk/202110-ioos-dmac">&amp;ldquo;Scalable, interoperable processing of water column sonar data for biological applications using the echopype Python package&amp;rdquo;&lt;/a>.&lt;/p>
&lt;p>&lt;strong>[10/05/2021]&lt;/strong> Wu-Jung gave the UW Data Science Seminar on &lt;a href="https://uw-echospace.github.io/talk/202110-uw-data-sci">&amp;ldquo;Building a toolbox for studying marine ecology using large ocean sonar datasets&amp;rdquo;&lt;/a>.&lt;/p>
&lt;p>&lt;strong>[09/21/2021]&lt;/strong> Wu-Jung and Linda successfully completed this summer&amp;rsquo;s fieldwork evaluating the use of an ADCP-equipped glider as a biological monitoring tool. Check out &lt;a href="https://oceanexplorer.noaa.gov/technology/development-partnerships/21adcp-gliders/welcome.html">NOAA Exploration&amp;rsquo;s coverage of this mission&lt;/a>!&lt;/p></description></item><item><title>Dynamic echo information guides flight in the big brown bat</title><link>https://uw-echospace.github.io/publication/2016-warnecke-etal-frontier-echo-flow/</link><pubDate>Tue, 01 Mar 2016 09:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/publication/2016-warnecke-etal-frontier-echo-flow/</guid><description/></item><item><title>Statistics of broadband echoes: application to acoustic estimates of numerical density of fish</title><link>https://uw-echospace.github.io/publication/2015-lee-stanton-joe-broadband/</link><pubDate>Tue, 01 Dec 2015 09:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/publication/2015-lee-stanton-joe-broadband/</guid><description/></item><item><title>Bats regulate biosonar based on the availability of visual information</title><link>https://uw-echospace.github.io/publication/2015-danilovich-etal-currbio-rousettus-light/</link><pubDate>Wed, 01 Jul 2015 09:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/publication/2015-danilovich-etal-currbio-rousettus-light/</guid><description/></item><item><title>Can the elongated hindwing tails of fluttering moths serve as false sonar targets to divert bat attacks?</title><link>https://uw-echospace.github.io/publication/2016-lee-moss-jasa-luna-moth/</link><pubDate>Fri, 01 May 2015 09:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/publication/2016-lee-moss-jasa-luna-moth/</guid><description/></item><item><title>Statistics of echoes from mixed assemblages of scatterers with different scattering amplitudes and numerical densities</title><link>https://uw-echospace.github.io/publication/2014-lee-stanton-joe-mixed/</link><pubDate>Mon, 13 Jan 2014 09:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/publication/2014-lee-stanton-joe-mixed/</guid><description/></item><item><title>Orientation dependence of broadband acoustic backscattering from live squid</title><link>https://uw-echospace.github.io/publication/2012-lee-etal-jasa-squid/</link><pubDate>Fri, 01 Jun 2012 09:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/publication/2012-lee-etal-jasa-squid/</guid><description/></item><item><title>Long-duration anesthetization of squid (Doryteuthis pealeii)</title><link>https://uw-echospace.github.io/publication/2010-mooney-etal-squid-sedation/</link><pubDate>Thu, 01 Apr 2010 09:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/publication/2010-mooney-etal-squid-sedation/</guid><description/></item><item><title>The acoustic field on the forehead of echolocating Atlantic bottlenose dolphins (Tursiops truncatus)</title><link>https://uw-echospace.github.io/publication/2010-au-etal-jasa-tursiops-suctioncup/</link><pubDate>Mon, 01 Mar 2010 09:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/publication/2010-au-etal-jasa-tursiops-suctioncup/</guid><description/></item></channel></rss>