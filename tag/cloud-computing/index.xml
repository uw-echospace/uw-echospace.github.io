<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cloud computing | Echospace @ UW</title><link>https://uw-echospace.github.io/tag/cloud-computing/</link><atom:link href="https://uw-echospace.github.io/tag/cloud-computing/index.xml" rel="self" type="application/rss+xml"/><description>cloud computing</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 24 Oct 2021 16:56:36 -0800</lastBuildDate><image><url>https://uw-echospace.github.io/images/icon_hu9a3e81eca2a83564e549c2cdc32b0637_27049_512x512_fill_lanczos_center_2.png</url><title>cloud computing</title><link>https://uw-echospace.github.io/tag/cloud-computing/</link></image><item><title>Machine learning in fisheries acoustics</title><link>https://uw-echospace.github.io/project/hake-ml/</link><pubDate>Sat, 01 May 2021 17:02:36 -0800</pubDate><guid>https://uw-echospace.github.io/project/hake-ml/</guid><description>&lt;p>
&lt;a href="https://storymaps.arcgis.com/stories/e245977def474bdba60952f30576908f" target="_blank" rel="noopener">Active acoustic data collected by echosounders&lt;/a> (high-frequency sonar systems) play a crucial role in marine ecological research and fisheries stock assessments. Recent technical advancements has further integrated echosounders onto many ocean observing platforms, leading to the rapid accumulation of echosounder data worldwide.&lt;/p>
&lt;p>In this project, we tackle the challenge of translating experiences from human experts into machine learning models capable of efficiently extracting biological information from large echosounder dataset. Using the rich dataset collected by the
&lt;a href="https://www.fisheries.noaa.gov/west-coast/science-data/joint-us-canada-integrated-ecosystem-and-pacific-hake-acoustic-trawl-survey" target="_blank" rel="noopener">Joint U.S.-Canada Integrated Ecosystem and Pacific Hake Acoustic Trawl Survey&lt;/a> dated back to early 2000s, we are developing deep learning models to automatically annotate echograms—color-coded visual representations of echo returns—with the presence of specific fish and zooplankton species or taxa.&lt;/p>
&lt;p>In the first stage of the project, we are focusing on developing
&lt;a href="https://uw-echospace.github.io/talk/202405-asa-ottawa-hake/">an echogram segmentation model to identify Pacific hake&lt;/a>, a keystone species and the largest fishery stock on the west coast of the US. Identifying hake on echograms is more challenging compared to many other fish species, due to their polymorphic appearance and diffused school boundaries. We found that neural networks' large learning capacity are well-suited to address these complexities. However, as in many other domains, organizing echosounder data with survey metadata and sorting expert annotations remains a significant bottleneck in fully leveraging these technologies.&lt;/p>
&lt;p>Moving forward, we aim to expand the model to include other ecologically and commercially important fish species in the California Current ecosystem, and incorporate other analytical methods, such as Bayesian inversion techniques, to improve acoustic data interpretation and biomass estimation accuracy.&lt;/p>
&lt;!-- To take full advantage of these large and complex new datasets, in this project we aim to combine the development of machine learning methodology with a cloud-based workflow to accelerate the extraction of biological information from fisheries acoustic data. Our group has developed and used [Echopype](https://echopype.readthedocs.io/en/stable/), a Raw Sonar Backscatter data parsing Python package, and [Echoregions](https://echoregions.readthedocs.io/en/latest/), an Echoview annotation data parsing Python package. Transferring data from Echoview and proprietary echosounder formats to Python data products enables seamless integration with a rich ecosystem of scientific computing tools developed by a vast community of open-source contributors, thus allowing us to use our data to train deep learning models to predict regions of interest in echograms.
&lt;img src="featured.png" alt="Fisheries Acoustics"> -->
&lt;p align="center">
&lt;img src="featured.png" alt="" style="width:1000px"/>
&lt;b>Echogram examples showing the deep learning model predicts regions similar to human expert annotations.&lt;/b>
&lt;/p>
&lt;p>This project is in close collaboration with the
&lt;a href="https://www.fisheries.noaa.gov/west-coast/sustainable-fisheries/fisheries-engineering-and-acoustic-technologies-team" target="_blank" rel="noopener">Fisheries Engineering and Acoustics Technology (FEAT) team&lt;/a> at the NOAA Fisheries
&lt;a href="https://www.fisheries.noaa.gov/about/northwest-fisheries-science-center" target="_blank" rel="noopener">Northwest Fisheries science center (NWFSC)&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Funding agency&lt;/strong>: NOAA Fisheries&lt;/p></description></item><item><title>Scalable, cloud-native processing of water column sonar data</title><link>https://uw-echospace.github.io/project_others/2021-cloud-workflows/</link><pubDate>Sun, 24 Oct 2021 16:56:36 -0800</pubDate><guid>https://uw-echospace.github.io/project_others/2021-cloud-workflows/</guid><description>&lt;p>Scientists commonly use active sonar systems to collect data about mid-trophic level animals like zooplankton and small fish, which play an important role in the marine ecosystems. Echosounders, or fish-finders, are high-frequency sonar systems that emit pulses of sound and record the reflections from animals, the seabed, and other objects. These instruments have been proven to be more efficient and effective for collecting data over a large survey area or a long time period than many other sampling methods, such as underwater imaging and net trawls. This technology has been widely adopted by the ocean science and commercial fishing communities and more recently has been integrated with autonomous vehicles, resulting in a massive amount of data. However, these datasets can be difficult to analyze and are often underutilized. We will address this issue by adopting and advancing data standards, developing a streamlined data processing workflow, and integrating open-source software tools that capitalize on recent advancements in cloud computing technologies to efficiently transform large quantities of ocean sonar data into information that is useful for exploring, monitoring, and managing living marine resources.&lt;/p>
&lt;p>&lt;strong>Funding agency&lt;/strong>: NOAA Office of Ocean Exploration and Research
&lt;a href="https://oceanexplorer.noaa.gov/news/oer-updates/2021/fy21-ffo-schedule.html" target="_blank" rel="noopener">FY2021 grants&lt;/a>&lt;/p></description></item><item><title>The open-source "Echostack" for flexible and scalable echosounder data processing</title><link>https://uw-echospace.github.io/project/echostack/</link><pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/project/echostack/</guid><description>&lt;p>
&lt;a href="https://storymaps.arcgis.com/stories/e245977def474bdba60952f30576908f" target="_blank" rel="noopener">Water column sonar data collected by echosounders&lt;/a> are essential for fisheries and marine ecosystem research, enabling the detection, classification, and quantification of fish and zooplankton from many different ocean observing platforms. However, the broad usage of these data has been hindered by the lack of modular software tools that allow flexible composition of data processing workflows that incorporate powerful analytical tools in the scientific Python ecosystem.&lt;/p>
&lt;p>We address this gap by developing &lt;strong>Echostack&lt;/strong>, a suite of open-source Python software packages that leverage existing distributed computing and cloud-interfacing libraries to support intuitive and scalable data access, processing, and interpretation. These tools can be used individually or orchestrated together, which we demonstrate in example use cases for a fisheries acoustic-trawl survey.&lt;/p>
&lt;p>Below is a summary of the Echostack package:&lt;/p>
&lt;p align="center">
&lt;img src="echostack_summary.png" alt="Echostack summary" style="width:1000px"/>
&lt;b>Bat call activity detected in two UBNA sites in 2022.&lt;/b>
&lt;/p>
&lt;p>For more information, check out each of the code repositories:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://github.com/OSOceanAcoustics/echopype" target="_blank" rel="noopener">Echopype&lt;/a>: performs data standardization and computation from raw instrument files to acoustic data products
&lt;ul>
&lt;li>Check out the
&lt;a href="https://doi.org/10.1093/icesjms/fsae133" target="_blank" rel="noopener">Echopype paper in ICES Journal of Marine Science&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;a href="https://github.com/OSOceanAcoustics/echopop" target="_blank" rel="noopener">Echopop&lt;/a>: generates acoustically derived biological estimates, such as abundance
&lt;ul>
&lt;li>Learn more on the
&lt;a href="../../project_others/echopop/">Echopop project page&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;a href="https://github.com/OSOceanAcoustics/echoshader" target="_blank" rel="noopener">Echoshader&lt;/a>: enables interactive acoustic data visualization and exploration&lt;/li>
&lt;li>
&lt;a href="https://github.com/OSOceanAcoustics/echoregions" target="_blank" rel="noopener">Echoregions&lt;/a>: interfaces acoustic data with machine learning developments&lt;/li>
&lt;li>
&lt;a href="https://github.com/OSOceanAcoustics/echodataflow" target="_blank" rel="noopener">Echodataflow&lt;/a>: workflow orchestration via text-based configuration “recipes” instead of code&lt;/li>
&lt;/ul>
&lt;p>These packages are accompanied by a set of data processing level definitions,
&lt;a href="https://github.com/OSOceanAcoustics/echolevels" target="_blank" rel="noopener">Echolevels&lt;/a>, which categorizes data products at different workflow stages to enhance data understanding and provenance tracking.&lt;/p>
&lt;p>Check out Wu-Jung&amp;rsquo;s talk at SciPy 2024 and the associated
&lt;a href="https://doi.org/10.25080/WXRH8633" target="_blank" rel="noopener">paper&lt;/a> in the proceedings!
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/YRFxMGisGww" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;p>&lt;strong>Funding&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>NOAA Fisheries&lt;/li>
&lt;li>NOAA Office of Ocean Exploration and Research
&lt;a href="https://oceanexplorer.noaa.gov/news/oer-updates/2021/fy21-ffo-schedule.html" target="_blank" rel="noopener">FY2021 grants&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>