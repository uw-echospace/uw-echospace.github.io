<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Project | Echospace @ UW</title><link>https://uw-echospace.github.io/project/</link><atom:link href="https://uw-echospace.github.io/project/index.xml" rel="self" type="application/rss+xml"/><description>Project</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 28 Mar 2024 17:02:36 -0800</lastBuildDate><image><url>https://uw-echospace.github.io/images/icon_hu9a3e81eca2a83564e549c2cdc32b0637_27049_512x512_fill_lanczos_center_2.png</url><title>Project</title><link>https://uw-echospace.github.io/project/</link></image><item><title>BOAT: Bridge to Ocean Acoustics and Technology</title><link>https://uw-echospace.github.io/project/boat/</link><pubDate>Thu, 28 Mar 2024 17:02:36 -0800</pubDate><guid>https://uw-echospace.github.io/project/boat/</guid><description>&lt;p>Ocean acoustics is an interdisciplinary field in which researchers focus on measuring, modeling, and understanding acoustic phenomena of oceanographical, geological, biological, and anthropogenic sources. However, despite its inherent interdisciplinary nature, Ocean acoustics research currently has limited presence in most US institutions and is typically viewed as a highly niched field.&lt;/p>
&lt;p>The
&lt;a href="https://boat-ocean-acoustics.github.io/" target="_blank" rel="noopener">Bridge to Ocean Acoustics and Technology (BOAT)&lt;/a> program aims to broaden access to ocean acoustics by empowering learners to explore and advance the field through collaboration and shared knowledge, focusing on:&lt;/p>
&lt;ul>
&lt;li>Developing open, executable, and web-hosted tutorials that encapsulate fundamental ocean acoustics knowledge and techniques as living documents.&lt;/li>
&lt;li>Growing the ocean acoustics education and research community through interactive and collaborative workshops.&lt;/li>
&lt;/ul>
&lt;p>In the current pilot phase, we will host two education workshops to provide a hands-on introduction to ocean acoustics—from fundamental concepts to real-world applications—using interactive
&lt;a href="https://jupyter.org/" target="_blank" rel="noopener">Jupyter notebooks&lt;/a>. We will cover topics broadly applicable to both active and passive acoustics, with hands-on experience using echosounder and hydrophone datasets.&lt;/p>
&lt;p>Our goal is to create open tutorials that can 1) be adapted to various educational settings, including regular university courses or summer workshops, and 2) serve as blueprints for further developing in-depth resources on specific ocean acoustics topics.&lt;/p>
&lt;p>See the
&lt;a href="https://boat-ocean-acoustics.github.io/" target="_blank" rel="noopener">BOAT website&lt;/a> for information for the two upcoming workshops in
&lt;a href="https://boat-ocean-acoustics.github.io/workshop_seattle.html" target="_blank" rel="noopener">Seattle&lt;/a> and
&lt;a href="https://boat-ocean-acoustics.github.io/workshop_new_orleans.html" target="_blank" rel="noopener">New Orleans&lt;/a>!&lt;/p>
&lt;p>&lt;strong>Funding Agency:&lt;/strong> Office of Naval Research, Ocean Acoustics Program&lt;/p></description></item><item><title>Passive acoustic monitoring in the Union Bay Natural Area</title><link>https://uw-echospace.github.io/project/ubna-pam/</link><pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/project/ubna-pam/</guid><description>&lt;p>Passive acoustic monitoring (PAM) has become a useful technique for monitoring soniferous animals in both terrestrial and marine habitats, and has in recent years been particularly bolstered by the broader availability and accessibility of low-cost recording devices.&lt;/p>
&lt;p>In this project, we deploy low-cost
&lt;a href="https://www.openacousticdevices.info/audiomoth" target="_blank" rel="noopener">AudioMoth&lt;/a> recorders at multiple sites in the
&lt;a href="https://botanicgardens.uw.edu/center-for-urban-horticulture/visit/union-bay-natural-area/" target="_blank" rel="noopener">Union Bay Natural Area&lt;/a>, right on the eastern edge of the University of Washington, Seattle campus. Since the project began in fall 2021, we have collected over 30 TB of recordings that are embedded with sounds from a wide variety of animals (e.g., birds, bats, frogs) and anthopogenic sources (e.g., airplanes, football stadium roars).&lt;/p>
&lt;p>Beyond generating a rich dataset, the project fieldwork and data analysis provides an accesible entry point for students to engage in real-world bioacoustics research, with hands-on data science and instrumentation opportunities.&lt;/p>
&lt;p>We have leveraged this dataset to investigate the
&lt;a href="https://uw-echospace.github.io/talk/202405-aditya-duty-cycle/">impact of duty cycle recording on bat monitoring&lt;/a> and support multiple capstone projects in the
&lt;a href="https://www.washington.edu/datasciencemasters/" target="_blank" rel="noopener">UW Data Sciene Master&amp;rsquo;s Program&lt;/a>, focused on developing open-source, cloud-hosted workflows for analyzing large PAM datasets. With these tools in place, going forward we plan to characterize the seasonal soundscape fluctuations in UBNA with respect to weather/climate events, and find opportunities to expand this effort to a community monitoring program in the Greater Seattle area.&lt;/p>
&lt;p align="center">
&lt;img src="featured.png" alt="UBNA bat activity grid" style="width:600px"/>
&lt;b>Bat call activity detected in two UBNA sites in 2022.&lt;/b>
&lt;/p>
&lt;p>&lt;strong>Funding&lt;/strong>:
&lt;a href="https://www.washington.edu/research/or/royalty-research-fund-rrf/" target="_blank" rel="noopener">UW Royalty Research Fund&lt;/a>&lt;/p></description></item><item><title>Machine learning in fisheries acoustics</title><link>https://uw-echospace.github.io/project/hake-ml/</link><pubDate>Sat, 01 May 2021 17:02:36 -0800</pubDate><guid>https://uw-echospace.github.io/project/hake-ml/</guid><description>&lt;p>
&lt;a href="https://storymaps.arcgis.com/stories/e245977def474bdba60952f30576908f" target="_blank" rel="noopener">Active acoustic data collected by echosounders&lt;/a> (high-frequency sonar systems) play a crucial role in marine ecological research and fisheries stock assessments. Recent technical advancements has further integrated echosounders onto many ocean observing platforms, leading to the rapid accumulation of echosounder data worldwide.&lt;/p>
&lt;p>In this project, we tackle the challenge of translating experiences from human experts into machine learning models capable of efficiently extracting biological information from large echosounder dataset. Using the rich dataset collected by the
&lt;a href="https://www.fisheries.noaa.gov/west-coast/science-data/joint-us-canada-integrated-ecosystem-and-pacific-hake-acoustic-trawl-survey" target="_blank" rel="noopener">Joint U.S.-Canada Integrated Ecosystem and Pacific Hake Acoustic Trawl Survey&lt;/a> dated back to early 2000s, we are developing deep learning models to automatically annotate echograms—color-coded visual representations of echo returns—with the presence of specific fish and zooplankton species or taxa.&lt;/p>
&lt;p>In the first stage of the project, we are focusing on developing
&lt;a href="https://uw-echospace.github.io/talk/202405-asa-ottawa-hake/">an echogram segmentation model to identify Pacific hake&lt;/a>, a keystone species and the largest fishery stock on the west coast of the US. Identifying hake on echograms is more challenging compared to many other fish species, due to their polymorphic appearance and diffused school boundaries. We found that neural networks' large learning capacity are well-suited to address these complexities. However, as in many other domains, organizing echosounder data with survey metadata and sorting expert annotations remains a significant bottleneck in fully leveraging these technologies.&lt;/p>
&lt;p>Moving forward, we aim to expand the model to include other ecologically and commercially important fish species in the California Current ecosystem, and incorporate other analytical methods, such as Bayesian inversion techniques, to improve acoustic data interpretation and biomass estimation accuracy.&lt;/p>
&lt;!-- To take full advantage of these large and complex new datasets, in this project we aim to combine the development of machine learning methodology with a cloud-based workflow to accelerate the extraction of biological information from fisheries acoustic data. Our group has developed and used [Echopype](https://echopype.readthedocs.io/en/stable/), a Raw Sonar Backscatter data parsing Python package, and [Echoregions](https://echoregions.readthedocs.io/en/latest/), an Echoview annotation data parsing Python package. Transferring data from Echoview and proprietary echosounder formats to Python data products enables seamless integration with a rich ecosystem of scientific computing tools developed by a vast community of open-source contributors, thus allowing us to use our data to train deep learning models to predict regions of interest in echograms.
&lt;img src="featured.png" alt="Fisheries Acoustics"> -->
&lt;p align="center">
&lt;img src="featured.png" alt="" style="width:1000px"/>
&lt;b>Echogram examples showing the deep learning model predicts regions similar to human expert annotations.&lt;/b>
&lt;/p>
&lt;p>This project is in close collaboration with the
&lt;a href="https://www.fisheries.noaa.gov/west-coast/sustainable-fisheries/fisheries-engineering-and-acoustic-technologies-team" target="_blank" rel="noopener">Fisheries Engineering and Acoustics Technology (FEAT) team&lt;/a> at the NOAA Fisheries
&lt;a href="https://www.fisheries.noaa.gov/about/northwest-fisheries-science-center" target="_blank" rel="noopener">Northwest Fisheries science center (NWFSC)&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Funding agency&lt;/strong>: NOAA Fisheries&lt;/p></description></item><item><title>The open-source "Echostack" for scalable, cloud-native processing of water column sonar data</title><link>https://uw-echospace.github.io/project/echostack/</link><pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/project/echostack/</guid><description>&lt;p>Water column sonar data collected by echosounders are essential for fisheries and marine ecosystem research, enabling the detection, classification, and quantification of fish and zooplankton from many different ocean observing platforms. However, the broad usage of these data has been hindered by the lack of modular software tools that allow flexible composition of data processing workflows that incorporate powerful analytical tools in the scientific Python ecosystem. We address this gap by developing &lt;strong>Echostack&lt;/strong>, a suite of open-source Python software packages that leverage existing distributed computing and cloud-interfacing libraries to support intuitive and scalable data access, processing, and interpretation. These tools can be used individually or orchestrated together, which we demonstrate in example use cases for a fisheries acoustic-trawl survey.&lt;/p>
&lt;p>Currently, the Echostack contains the following packages:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://github.com/OSOceanAcoustics/echopype" target="_blank" rel="noopener">Echopype&lt;/a>: performs data standardization and computation from raw instrument files to acoustic data products
&lt;ul>
&lt;li>Check out the
&lt;a href="https://doi.org/10.1093/icesjms/fsae133" target="_blank" rel="noopener">Echopype paper in ICES Journal of Marine Science&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;a href="https://github.com/OSOceanAcoustics/echopop" target="_blank" rel="noopener">Echopop&lt;/a>: generates acoustically derived biological estimates, such as abundance
&lt;ul>
&lt;li>Learn more on
&lt;a href="../../project_others/echopop/">Echopop project page&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;a href="https://github.com/OSOceanAcoustics/echoshader" target="_blank" rel="noopener">Echoshader&lt;/a>: enables interactive acoustic data visualization and exploration&lt;/li>
&lt;li>
&lt;a href="https://github.com/OSOceanAcoustics/echoregions" target="_blank" rel="noopener">Echoregions&lt;/a>: interfaces acoustic data with machine learning developments&lt;/li>
&lt;li>
&lt;a href="https://github.com/OSOceanAcoustics/echodataflow" target="_blank" rel="noopener">Echodataflow&lt;/a>: workflow orchestration via text-based configuration “recipes” instead of code&lt;/li>
&lt;/ul>
&lt;p>These packages are accompanied by a set of data processing level definitions,
&lt;a href="https://github.com/OSOceanAcoustics/echolevels" target="_blank" rel="noopener">Echolevels&lt;/a>, which categorizes data products at different workflow stages to enhance data understanding and provenance tracking.&lt;/p>
&lt;p>Check out Wu-Jung&amp;rsquo;s talk at SciPy 2024 and the associated
&lt;a href="https://doi.org/10.25080/WXRH8633" target="_blank" rel="noopener">proceeding paper&lt;/a>!
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/YRFxMGisGww" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;p>&lt;strong>Funding&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>NOAA Fisheries&lt;/li>
&lt;li>NOAA Office of Ocean Exploration and Research
&lt;a href="https://oceanexplorer.noaa.gov/news/oer-updates/2021/fy21-ffo-schedule.html" target="_blank" rel="noopener">FY2021 grants&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>ADCP-equipped underwater glider as a distributed biological sensing tool</title><link>https://uw-echospace.github.io/project/2020-glider-adcp/</link><pubDate>Tue, 01 Sep 2020 17:02:36 -0800</pubDate><guid>https://uw-echospace.github.io/project/2020-glider-adcp/</guid><description>&lt;p>Mid-trophic level animals, such as zooplankton and fish, are keystone organisms in the marine ecosystem and play a critical role in the economy and our food supply chain. However, our understanding of these animals, particularly those in the pelagic zones, is severely limited, due to the lack of tools that cab . This gap of knowledge has greatly impeded our ability in making informed policy decisions to support sustainable resource management. The root cause of this problem is the lack of tools that can collect information about these animals at large temporal and spatial scales comparable to other physical, chemical, and lower-trophic biological (e.g., chlorophyll) oceanographic variables.&lt;/p>
&lt;p>Gliders have provided unparalleled mobile, persistent access to deep, remote ocean environments at a fraction of the cost of a research vessel. Taking advantage of this unique capability, in this project we aim to develop sampling strategies and data analysis methodologies to enable distributed long-term observation of mid-trophic marine organisms using
&lt;a href="https://apl.uw.edu/project/project.php?id=seaglider_auv" target="_blank" rel="noopener">Seagliders&lt;/a> equipped with acoustic Doppler current profilers (ADCPs).&lt;/p>
&lt;p>&lt;strong>Funding agency&lt;/strong>: NOAA Office of Ocean Exploration and Research
&lt;a href="https://oceanexplorer.noaa.gov/news/oer-updates/2020/fy20-ffo-schedule.html" target="_blank" rel="noopener">FY2020 grants&lt;/a>&lt;/p></description></item><item><title>Pattern discovery from long-term echosounder time series</title><link>https://uw-echospace.github.io/project/2019-ooi-mtx-decomp/</link><pubDate>Tue, 01 Jan 2019 14:50:00 -0800</pubDate><guid>https://uw-echospace.github.io/project/2019-ooi-mtx-decomp/</guid><description>&lt;p>&lt;strong>Funding agency&lt;/strong>: National Science Foundation
&lt;a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1849930&amp;amp;HistoricalAwards=false" target="_blank" rel="noopener">Award #1849930&lt;/a>&lt;/p></description></item><item><title>Echo Statistics</title><link>https://uw-echospace.github.io/project/echo-stat-tutorial/</link><pubDate>Sat, 01 Dec 2018 00:00:00 -0800</pubDate><guid>https://uw-echospace.github.io/project/echo-stat-tutorial/</guid><description>&lt;p>For a 2018 tutorial I published with
&lt;a href="https://www2.whoi.edu/staff/tstanton/" target="_blank" rel="noopener">Tim Stanton&lt;/a> and
&lt;a href="https://www.linkedin.com/in/kyungmin-baik-098156149/" target="_blank" rel="noopener">Kyungmin Baik&lt;/a> in
the Journal of the Acoustical Society of America (JASA):&lt;/p>
&lt;p>&lt;strong>Echo statistics associated with discrete scatterers: A tutorial on physics-based methods&lt;/strong>. JASA 144(6): 3124–3171; &lt;a href="https://doi.org/10.1121/1.5052255">https://doi.org/10.1121/1.5052255&lt;/a>&lt;/p>
&lt;p>we provided the Matlab code to reproduce all figures in two forms:&lt;/p>
&lt;ul>
&lt;li>a &lt;em>frozen&lt;/em> version archived with the paper, and&lt;/li>
&lt;li>a GitHub
&lt;a href="https://github.com/leewujung/echo-stats-tutorial" target="_blank" rel="noopener">repository&lt;/a> minted with a
&lt;a href="https://doi.org/10.5281/zenodo.2458776" target="_blank" rel="noopener">DOI from Zenodo&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>This way we can keep the code &amp;ldquo;alive&amp;rdquo; on GitHub but also has a convenient snapshot of the code at the time of the tutorial publication.&lt;/p></description></item><item><title>Modeling sound propagation in the head of toothed whales</title><link>https://uw-echospace.github.io/project/echolocation-comsol/</link><pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/project/echolocation-comsol/</guid><description>&lt;p>Toothed whales, including species such as porpoises, dolphins, orca, and sperm whale, possess highly specialized anatomical structures in the head to support their biosonar systems - echolocation - through millions of years of evoluation. These animals have the remarkable ability to detect and track small targets over long distance and discriminate between minute differences between targets using echolocation, with performance often surpassing that of current human-made sonar systems. However, many questions remain in how exactly the unusual anatomical structures in the head of toothed whales are orchestrated to support such performance.&lt;/p>
&lt;p>As part of a Multidisciplinary University Research Initiative (MURI) project, we use finite element modeling techniques in combination with volumetric representations derived from computed tomography (CT) scans to predict the head-related transfer functions (HRTFs) of a dolphin head. The HRTFs summarizes the influence of the head to sounds propagating to the ears. We use HRTFs as a biologically meaningful proxy to provide a physics-based mechanistic understanding of the sound transduction processes.&lt;/p>
&lt;!-- TODO: link ASA 2023 talk -->
&lt;p>&lt;strong>Funding&lt;/strong>: Office of Naval Research, Multidisciplinary University Research Initiative (MURI) program&lt;/p></description></item><item><title>Target search and discrimination by echolocating toothed whales</title><link>https://uw-echospace.github.io/project/echolocation-search/</link><pubDate>Thu, 01 Jan 1970 00:33:38 +0000</pubDate><guid>https://uw-echospace.github.io/project/echolocation-search/</guid><description>&lt;p>Echolocating animals effortlessly navigate, hunt, and interact with their environment, despite cluttered and noisy return signals. Blind expert human echolocators prove that this capacity does not depend exclusively on biological specializations unique to particular species. This project is an integrated component of a larger collaborative Multidisciplinary University Research Initiative (MURI) project focused on active sensing in echolocating marine mammals and humans. The MURI team use both toothed whales (odontocetes) and humans as model systems to identify the neural mechanisms that extract echo-acoustic information and the brain networks that build and learn robust, invariant representations of auditory objects in complex auditory scenes.&lt;/p>
&lt;p>In Echospace, we undertake two interconnected components of this project:&lt;/p>
&lt;ul>
&lt;li>Model the echolocation-based target search by toothed whales as an information-seeking behavior by extending the &lt;em>infotaxis&lt;/em> algorithm originally formulated in moth odor tracking problems into an &lt;em>active sensing&lt;/em> context&lt;/li>
&lt;/ul>
&lt;!-- TODO: link ASA 2019 talk -->
&lt;ul>
&lt;li>Conduct and analyze the coupled acoustic sampling and movement behaviors of an echolocating harbor porpoise in a target discrimination experiment&lt;/li>
&lt;/ul>
&lt;!-- TODO: link ASA 2021, 2023 talks -->
&lt;p>&lt;strong>Funding agency&lt;/strong>: Office of Naval Research, Multidisciplinary University Research Initiative (MURI) program&lt;/p></description></item></channel></rss>