<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Project | Echospace @ UW</title><link>https://uw-echospace.github.io/project/</link><atom:link href="https://uw-echospace.github.io/project/index.xml" rel="self" type="application/rss+xml"/><description>Project</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 28 Mar 2024 17:02:36 -0800</lastBuildDate><image><url>https://uw-echospace.github.io/images/icon_hu9a3e81eca2a83564e549c2cdc32b0637_27049_512x512_fill_lanczos_center_2.png</url><title>Project</title><link>https://uw-echospace.github.io/project/</link></image><item><title>BOAT: Bridge to Ocean Acoustics and Technology</title><link>https://uw-echospace.github.io/project/boat/</link><pubDate>Thu, 28 Mar 2024 17:02:36 -0800</pubDate><guid>https://uw-echospace.github.io/project/boat/</guid><description>&lt;p>
&lt;a href="https://uw-echospace.github.io/author/wu-jung-lee">Wu-Jung Lee&lt;/a> (APL),
&lt;a href="https://uw-echospace.github.io/author/valentina-staneva">Valentina Staneva&lt;/a> (eScience Institute), and
&lt;a href="https://www.apl.washington.edu/people/profile.php?last_name=Hefner&amp;amp;first_name=Todd" target="_blank" rel="noopener">Todd Hefner&lt;/a> (APL) were awarded a grant by the Office of Naval Research to democratize ocean acoustics education and research training. The new program BOAT: Bridge to Ocean Acoustics and Technology will develop open source education curriculum&amp;amp;tutorials and interactive training workshops that aim to prepare students from diverse backgrounds (biology, physics, electrical engineering, math, etc) for doing research in the interdisciplinary field of Ocean Acoustics. The modules will cover topics ranging from the scientific theory (physics) of scattering of the seabed and marine organisms, practical applications in oceanography and fisheries, to foundational topics in statistics, machine learning, and data science workflows which all are indispensable in navigating today’s data-rich science. They will be provided in the form of
&lt;a href="https://ebooks.iospress.nl/publication/42900" target="_blank" rel="noopener">Jupyter Notebooks&lt;/a> with interactive components, which can be adapted and ingested in various settings such as a university course or a summer workshop, and will serve as blueprints for building resources on other ocean acoustics topics. The goal is to build a community of practitioners who build and share knowledge on best practices on how to teach and train new scientists in the field. For those interested in learning more about the program and/or exploring potential collaborations, please, reach out to the PIs at emails: &lt;code>leewj@uw.edu&lt;/code>, &lt;code>vms16@uw.edu&lt;/code>, &lt;code>bth3@uw.edu&lt;/code>.&lt;/p>
&lt;p>&lt;strong>Funding Agency:&lt;/strong> Office of Naval Research&lt;/p></description></item><item><title>Passive acoustic monitoring of bats in the Union Bay Natural Area</title><link>https://uw-echospace.github.io/project/ubna-pam/</link><pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/project/ubna-pam/</guid><description>&lt;p>Understanding how echolocators make use of sound to navigate their surroundings has great potential for informing the design of advanced acoustic sensing technologies. Although much has already been studied in laboratory experiments, technology has just started catching up to allow researchers to study echolocation-related processes in the wild. Advancements in passive acoustic monitoring tools have made it affordable to conduct long-term acoustic surveys on animals in the wild. Echolocators, like bats, are well-suited for monitoring using simple passive acoustic techniques because of their use of acoustics and navgitaion in air.&lt;/p>
&lt;p>In this project, we sought to collect long-term acoustic data using Audiomoths from an urban natural area called the Union Bay Natural Area at the University of Washington. With this data, we hope to uncover questions on how environmental conditions influence how bats choose to forage.&lt;/p>
&lt;p>&lt;strong>Funding&lt;/strong>:
&lt;a href="https://www.washington.edu/research/or/royalty-research-fund-rrf/" target="_blank" rel="noopener">UW Royalty Research Fund&lt;/a>&lt;/p></description></item><item><title>Machine learning in fisheries acoustics</title><link>https://uw-echospace.github.io/project/2021-hake-workflow/</link><pubDate>Sat, 01 May 2021 17:02:36 -0800</pubDate><guid>https://uw-echospace.github.io/project/2021-hake-workflow/</guid><description>&lt;p>Active acoustic data collected with scientific echosounders from acoustic surveys have become essential for stock assessment in fisheries resources management. Over the past three decades, a wide suite of physics-based acoustic scattering models were developed to allow translating acoustic observations to biological quantities, such as biomass and abundance, for different marine organisms. In parallel, quantitative scientific echosounders have progressed from specialized equipment to one of the standard instruments on fisheries survey vessels.&lt;/p>
&lt;p>To take full advantage of these large and complex new datasets, in this project we aim to combine the development of machine learning methodology with a cloud-based workflow to accelerate the extraction of biological information from fisheries acoustic data. Our group has developed and used
&lt;a href="https://echopype.readthedocs.io/en/stable/" target="_blank" rel="noopener">Echopype&lt;/a>, a Raw Sonar Backscatter data parsing Python package, and
&lt;a href="https://echoregions.readthedocs.io/en/latest/" target="_blank" rel="noopener">Echoregions&lt;/a>, an Echoview annotation data parsing Python package. Transferring data from Echoview and proprietary echosounder formats to Python data products enables seamless integration with a rich ecosystem of scientific computing tools developed by a vast community of open-source contributors, thus allowing us to use our data to train deep learning models to predict regions of interest in echograms.&lt;/p>
&lt;img src="featured.png" alt="Fisheries Acoustics">
&lt;p>This project is in close collaboration with the
&lt;a href="https://www.fisheries.noaa.gov/west-coast/sustainable-fisheries/fisheries-engineering-and-acoustic-technologies-team" target="_blank" rel="noopener">Fisheries Engineering and Acoustics Technology (FEAT) team&lt;/a> at the NOAA Fisheries
&lt;a href="https://www.fisheries.noaa.gov/about/northwest-fisheries-science-center" target="_blank" rel="noopener">Northwest Fisheries science center (NWFSC)&lt;/a> and uses data collected in the past 20 years off the west coast of the U.S. from the
&lt;a href="https://www.fisheries.noaa.gov/west-coast/science-data/joint-us-canada-integrated-ecosystem-and-pacific-hake-acoustic-trawl-survey" target="_blank" rel="noopener">Joint U.S.-Canada Integrated Ecosystem and Pacific Hake Acoustic-Trawl Survey&lt;/a> (aka the &amp;ldquo;Hake survey&amp;rdquo;).&lt;/p>
&lt;p>&lt;strong>Funding agency&lt;/strong>: NOAA Fisheries&lt;/p></description></item><item><title>The open-source "Echostack" for scalable, cloud-native processing of water column sonar data</title><link>https://uw-echospace.github.io/project/echostack/</link><pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/project/echostack/</guid><description>&lt;p>Water column sonar data collected by echosounders are essential for fisheries and marine ecosystem research, enabling the detection, classification, and quantification of fish and zooplankton from many different ocean observing platforms. However, the broad usage of these data has been hindered by the lack of modular software tools that allow flexible composition of data processing workflows that incorporate powerful analytical tools in the scientific Python ecosystem. We address this gap by developing &lt;strong>Echostack&lt;/strong>, a suite of open-source Python software packages that leverage existing distributed computing and cloud-interfacing libraries to support intuitive and scalable data access, processing, and interpretation. These tools can be used individually or orchestrated together, which we demonstrate in example use cases for a fisheries acoustic-trawl survey.&lt;/p>
&lt;p>Currently, the Echostack contains the following packages:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://github.com/OSOceanAcoustics/echopype" target="_blank" rel="noopener">Echopype&lt;/a>: performs data standardization and computation from raw instrument files to acoustic data products
&lt;ul>
&lt;li>Check out the
&lt;a href="https://doi.org/10.1093/icesjms/fsae133" target="_blank" rel="noopener">Echopype paper in ICES Journal of Marine Science&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;a href="https://github.com/OSOceanAcoustics/echopop" target="_blank" rel="noopener">Echopop&lt;/a>: generates acoustically derived biological estimates, such as abundance
&lt;ul>
&lt;li>Learn more on
&lt;a href="../../project_others/echopop/">Echopop project page&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;a href="https://github.com/OSOceanAcoustics/echoshader" target="_blank" rel="noopener">Echoshader&lt;/a>: enables interactive acoustic data visualization and exploration&lt;/li>
&lt;li>
&lt;a href="https://github.com/OSOceanAcoustics/echoregions" target="_blank" rel="noopener">Echoregions&lt;/a>: interfaces acoustic data with machine learning developments&lt;/li>
&lt;li>
&lt;a href="https://github.com/OSOceanAcoustics/echodataflow" target="_blank" rel="noopener">Echodataflow&lt;/a>: workflow orchestration via text-based configuration “recipes” instead of code&lt;/li>
&lt;/ul>
&lt;p>These packages are accompanied by a set of data processing level definitions,
&lt;a href="https://github.com/OSOceanAcoustics/echolevels" target="_blank" rel="noopener">Echolevels&lt;/a>, which categorizes data products at different workflow stages to enhance data understanding and provenance tracking.&lt;/p>
&lt;p>Check out Wu-Jung&amp;rsquo;s talk at SciPy 2024 and the associated
&lt;a href="https://doi.org/10.25080/WXRH8633" target="_blank" rel="noopener">proceeding paper&lt;/a>!
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/YRFxMGisGww" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;p>&lt;strong>Funding&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>NOAA Fisheries&lt;/li>
&lt;li>NOAA Office of Ocean Exploration and Research
&lt;a href="https://oceanexplorer.noaa.gov/news/oer-updates/2021/fy21-ffo-schedule.html" target="_blank" rel="noopener">FY2021 grants&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>ADCP-equipped underwater glider as a distributed biological sensing tool</title><link>https://uw-echospace.github.io/project/2020-glider-adcp/</link><pubDate>Tue, 01 Sep 2020 17:02:36 -0800</pubDate><guid>https://uw-echospace.github.io/project/2020-glider-adcp/</guid><description>&lt;p>Mid-trophic level animals, such as zooplankton and fish, are keystone organisms in the marine ecosystem and play a critical role in the economy and our food supply chain. However, our understanding of these animals, particularly those in the pelagic zones, is severely limited, due to the lack of tools that cab . This gap of knowledge has greatly impeded our ability in making informed policy decisions to support sustainable resource management. The root cause of this problem is the lack of tools that can collect information about these animals at large temporal and spatial scales comparable to other physical, chemical, and lower-trophic biological (e.g., chlorophyll) oceanographic variables.&lt;/p>
&lt;p>Gliders have provided unparalleled mobile, persistent access to deep, remote ocean environments at a fraction of the cost of a research vessel. Taking advantage of this unique capability, in this project we aim to develop sampling strategies and data analysis methodologies to enable distributed long-term observation of mid-trophic marine organisms using
&lt;a href="https://apl.uw.edu/project/project.php?id=seaglider_auv" target="_blank" rel="noopener">Seagliders&lt;/a> equipped with acoustic Doppler current profilers (ADCPs).&lt;/p>
&lt;p>&lt;strong>Funding agency&lt;/strong>: NOAA Office of Ocean Exploration and Research
&lt;a href="https://oceanexplorer.noaa.gov/news/oer-updates/2020/fy20-ffo-schedule.html" target="_blank" rel="noopener">FY2020 grants&lt;/a>&lt;/p></description></item><item><title>Pattern discovery from long-term echosounder time series</title><link>https://uw-echospace.github.io/project/2019-ooi-mtx-decomp/</link><pubDate>Tue, 01 Jan 2019 14:50:00 -0800</pubDate><guid>https://uw-echospace.github.io/project/2019-ooi-mtx-decomp/</guid><description>&lt;p>&lt;strong>Funding agency&lt;/strong>: National Science Foundation
&lt;a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1849930&amp;amp;HistoricalAwards=false" target="_blank" rel="noopener">Award #1849930&lt;/a>&lt;/p></description></item><item><title>Echo Statistics</title><link>https://uw-echospace.github.io/project/echo-stat-tutorial/</link><pubDate>Sat, 01 Dec 2018 00:00:00 -0800</pubDate><guid>https://uw-echospace.github.io/project/echo-stat-tutorial/</guid><description>&lt;p>For a 2018 tutorial I published with
&lt;a href="https://www2.whoi.edu/staff/tstanton/" target="_blank" rel="noopener">Tim Stanton&lt;/a> and
&lt;a href="https://www.linkedin.com/in/kyungmin-baik-098156149/" target="_blank" rel="noopener">Kyungmin Baik&lt;/a> in
the Journal of the Acoustical Society of America (JASA):&lt;/p>
&lt;p>&lt;strong>Echo statistics associated with discrete scatterers: A tutorial on physics-based methods&lt;/strong>. JASA 144(6): 3124–3171; &lt;a href="https://doi.org/10.1121/1.5052255">https://doi.org/10.1121/1.5052255&lt;/a>&lt;/p>
&lt;p>we provided the Matlab code to reproduce all figures in two forms:&lt;/p>
&lt;ul>
&lt;li>a &lt;em>frozen&lt;/em> version archived with the paper, and&lt;/li>
&lt;li>a GitHub
&lt;a href="https://github.com/leewujung/echo-stats-tutorial" target="_blank" rel="noopener">repository&lt;/a> minted with a
&lt;a href="https://doi.org/10.5281/zenodo.2458776" target="_blank" rel="noopener">DOI from Zenodo&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>This way we can keep the code &amp;ldquo;alive&amp;rdquo; on GitHub but also has a convenient snapshot of the code at the time of the tutorial publication.&lt;/p></description></item><item><title>Modeling sound propagation in the head of toothed whales</title><link>https://uw-echospace.github.io/project/echolocation-comsol/</link><pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate><guid>https://uw-echospace.github.io/project/echolocation-comsol/</guid><description>&lt;p>Toothed whales, including species such as porpoises, dolphins, orca, and sperm whale, possess highly specialized anatomical structures in the head to support their biosonar systems - echolocation - through millions of years of evoluation. These animals have the remarkable ability to detect and track small targets over long distance and discriminate between minute differences between targets using echolocation, with performance often surpassing that of current human-made sonar systems. However, many questions remain in how exactly the unusual anatomical structures in the head of toothed whales are orchestrated to support such performance.&lt;/p>
&lt;p>As part of a Multidisciplinary University Research Initiative (MURI) project, we use finite element modeling techniques in combination with volumetric representations derived from computed tomography (CT) scans to predict the head-related transfer functions (HRTFs) of a dolphin head. The HRTFs summarizes the influence of the head to sounds propagating to the ears. We use HRTFs as a biologically meaningful proxy to provide a physics-based mechanistic understanding of the sound transduction processes.&lt;/p>
&lt;!-- TODO: link ASA 2023 talk -->
&lt;p>&lt;strong>Funding&lt;/strong>: Office of Naval Research, Multidisciplinary University Research Initiative (MURI) program&lt;/p></description></item><item><title>Target search and discrimination by echolocating toothed whales</title><link>https://uw-echospace.github.io/project/echolocation-search/</link><pubDate>Thu, 01 Jan 1970 00:33:38 +0000</pubDate><guid>https://uw-echospace.github.io/project/echolocation-search/</guid><description>&lt;p>Echolocating animals effortlessly navigate, hunt, and interact with their environment, despite cluttered and noisy return signals. Blind expert human echolocators prove that this capacity does not depend exclusively on biological specializations unique to particular species. This project is an integrated component of a larger collaborative Multidisciplinary University Research Initiative (MURI) project focused on active sensing in echolocating marine mammals and humans. The MURI team use both toothed whales (odontocetes) and humans as model systems to identify the neural mechanisms that extract echo-acoustic information and the brain networks that build and learn robust, invariant representations of auditory objects in complex auditory scenes.&lt;/p>
&lt;p>In Echospace, we undertake two interconnected components of this project:&lt;/p>
&lt;ul>
&lt;li>Model the echolocation-based target search by toothed whales as an information-seeking behavior by extending the &lt;em>infotaxis&lt;/em> algorithm originally formulated in moth odor tracking problems into an &lt;em>active sensing&lt;/em> context&lt;/li>
&lt;/ul>
&lt;!-- TODO: link ASA 2019 talk -->
&lt;ul>
&lt;li>Conduct and analyze the coupled acoustic sampling and movement behaviors of an echolocating harbor porpoise in a target discrimination experiment&lt;/li>
&lt;/ul>
&lt;!-- TODO: link ASA 2021, 2023 talks -->
&lt;p>&lt;strong>Funding agency&lt;/strong>: Office of Naval Research, Multidisciplinary University Research Initiative (MURI) program&lt;/p></description></item></channel></rss>